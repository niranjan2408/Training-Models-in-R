library(caTools)
library(corpcor)
library(car)
library(psych)
library(ROCR)
library(dplyr)
library(doParallel)
library(pROC)
registerDoParallel(core=4)
par(mfrow=c(1,2))
OrgData_CrdCard <-read.csv(choose.files())
OrgData_CrdCard <-OrgData_CrdCard[-1]
str(OrgData_CrdCard)
summary(OrgData_CrdCard)
sum(is.na(OrgData_CrdCard))
split<- sample.split(OrgData_CrdCard$active,SplitRatio = 0.7)
train_data<-subset(OrgData_CrdCard,split==TRUE)
test_data<-subset(OrgData_CrdCard,split==FALSE)
model<-glm(train_data$card~.,train_data,family = "binomial")
summary(model)
train_datal<-train_data[,c(1,2,4,5,9,12)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
#created variable prob and saved model3 predicted probability values for card
prob1 <- predict(model1,train_datal,type="response")
#created confusion matrix of predicted card probability values with cut off 0.5 and original card data
confusion1<-table(prob1>0.5,train_datal$card)
confusion1
Accuracy1<-sum(diag(confusion1)/sum(confusion1))
Accuracy1
#created variable (pred_values) with if predicted p value is greater than 0.5 put "yes" else "no"
pred_values1 <- ifelse(prob1>=0.5,"yes","no")
#Added column of (pred_values) to the original data set
train_datal[,"pred_values1"] <- pred_values1
View(train_datal[,c(1,7)]) #Just to compare actual data and predicted data
#Receiver Operating Characteristic (ROC) Curve Analysis
ROCR_Prediction1<-prediction(prob1,train_datal$card)
ROCR_Performance1<-performance(ROCR_Prediction1,'tpr','fpr')
plot(ROCR_Performance1,colorize=T,text.adj=c(-0.2,1.7),print.cutoffs.at=seq(0.1,by=0.1))
str(ROCR_Performance1)
ROCR_cutoff1 <- data.frame(cut_off = ROCR_Performance@alpha.values[[1]],fpr=ROCR_Performance@x.values,tpr=ROCR_Performance@y.values)
ROCR_cutoff1 <- data.frame(cut_off = ROCR_Performance1@alpha.values[[1]],fpr=ROCR_Performance1@x.values,tpr=ROCR_Performance1@y.values)
colnames(ROCR_cutoff1) <- c("cut_off","FPR","TPR")
ROCR_cutoff1<-round(ROCR_cutoff1,3)
View(ROCR_cutoff1)
confusion2<-table(prob1>0.25,train_datal$card)
confusion2
Accuracy<-sum(diag(confusion2)/sum(confusion2))
Accuracy
table(train_data$card)
714/923
library(caTools)
library(corpcor)
library(car)
library(psych)
library(ROCR)
library(dplyr)
library(doParallel)
library(pROC)
registerDoParallel(core=4)
par(mfrow=c(1,2))
OrgData_CrdCard <-read.csv(choose.files())
OrgData_CrdCard <-OrgData_CrdCard[-1]
str(OrgData_CrdCard)
summary(OrgData_CrdCard)
sum(is.na(OrgData_CrdCard))
split<- sample.split(OrgData_CrdCard$active,SplitRatio = 0.7)
train_data<-subset(OrgData_CrdCard,split==TRUE)
test_data<-subset(OrgData_CrdCard,split==FALSE)
model<-glm(train_data$card~.,train_data,family = "binomial")
summary(model)
train_datal<-train_data[,c(1,2,4,5,9,12)]
#checked colinearity of selected variables
pairs.panels(train_datal, col="red")
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
View(OrgData_CrdCard)
train_datal<-train_data[,c(1,2,4,5,6,9)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,2,4,5,9)]
#checked colinearity of selected variables
pairs.panels(train_datal, col="red")
cor2pcor(cor(train_datal))
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
pairs.panels(test_data, col="red")
train_datal<-train_data[,c(1,2,3,4,5,6,9,10,11,12)]
#checked colinearity of selected variables
pairs.panels(train_datal, col="red")
train_datal<-train_data[,c(1,2,3,4,5,6,9,10,12)]
#checked colinearity of selected variables
pairs.panels(train_datal, col="red")
cor2pcor(cor(train_datal))
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
model<-glm(train_data$card~.,train_data,family = "binomial")
summary(model)
cor(train_data$card,train_data$owner)
train_datal<-train_data[,c(1,2,3,4,5,6,7,8,9,10,11)]
#checked colinearity of selected variables
pairs.panels(train_datal, col="red")
cor2pcor(cor(train_datal))
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,2,3,4,5,6,7,8,9,10)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,2,3,4,5,6,7,8,9)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,2,3,4,5,6,7,8)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
split<- sample.split(OrgData_CrdCard$active,SplitRatio = 0.7)
train_data<-subset(OrgData_CrdCard,split==TRUE)
test_data<-subset(OrgData_CrdCard,split==FALSE)
train_datal<-train_data[,c(1,2,3,4,5,6,7,8)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,2,4,6)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
split<- sample.split(OrgData_CrdCard$active,SplitRatio = 0.7)
train_data<-subset(OrgData_CrdCard,split==TRUE)
test_data<-subset(OrgData_CrdCard,split==FALSE)
train_datal<-train_data[,c(1,2,4,6)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,5,6,9)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,5,9)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
train_datal<-train_data[,c(1,5,2)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
split<- sample.split(OrgData_CrdCard$active,SplitRatio = 0.7)
train_data<-subset(OrgData_CrdCard,split==TRUE)
test_data<-subset(OrgData_CrdCard,split==FALSE)
train_datal<-train_data[,c(1,5,2)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
split<- sample.split(OrgData_CrdCard$active,SplitRatio = 0.7)
train_data<-subset(OrgData_CrdCard,split==TRUE)
test_data<-subset(OrgData_CrdCard,split==FALSE)
train_datal<-train_data[,c(1,5,2)]
#checked colinearity of selected variables
#pairs.panels(train_datal, col="red")
#cor2pcor(cor(train_datal))
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
#created variable prob and saved model3 predicted probability values for card
prob1 <- predict(model1,train_datal,type="response")
#created confusion matrix of predicted card probability values with cut off 0.5 and original card data
confusion1<-table(prob1>0.5,train_datal$card)
confusion1
Accuracy1<-sum(diag(confusion1)/sum(confusion1))
Accuracy1
#created variable (pred_values) with if predicted p value is greater than 0.5 put "yes" else "no"
pred_values1 <- ifelse(prob1>=0.5,"yes","no")
#Added column of (pred_values) to the original data set
train_datal[,"pred_values1"] <- pred_values1
View(train_datal[,c(1,7)]) #Just to compare actual data and predicted data
#Receiver Operating Characteristic (ROC) Curve Analysis
ROCR_Prediction1<-prediction(prob1,train_datal$card)
ROCR_Performance1<-performance(ROCR_Prediction1,'tpr','fpr')
plot(ROCR_Performance1,colorize=T,text.adj=c(-0.2,1.7),print.cutoffs.at=seq(0.1,by=0.1))
str(ROCR_Performance1)
ROCR_cutoff1 <- data.frame(cut_off = ROCR_Performance1@alpha.values[[1]],fpr=ROCR_Performance1@x.values,tpr=ROCR_Performance1@y.values)
colnames(ROCR_cutoff1) <- c("cut_off","FPR","TPR")
ROCR_cutoff1<-round(ROCR_cutoff1,3)
confusion2<-table(prob1>0.25,train_datal$card)
confusion2
confusion2<-table(prob1>0.4,train_datal$card)
confusion2
confusion2<-table(prob1>0.2,train_datal$card)
confusion2
confusion2<-table(prob1>0.3,train_datal$card)
confusion2
confusion2<-table(prob1>0.25,train_datal$card)
confusion2
confusion2<-table(prob1>0.22,train_datal$card)
confusion2
confusion2<-table(prob1>0.23,train_datal$card)
confusion2
#Receiver Operating Characteristic (ROC) Curve Analysis
ROCR_Prediction1<-prediction(prob1,train_datal$card)
ROCR_Performance1<-performance(ROCR_Prediction1,'tpr','fpr')
Accuracy<-sum(diag(confusion2)/sum(confusion2))
Accuracy
train_datal<-test_data[,c(1,5,2)]
model1<-glm(train_datal$card~.,train_datal,family = "binomial")
summary(model1)
#created variable prob and saved model3 predicted probability values for card
prob1 <- predict(model1,train_datal,type="response")
#created confusion matrix of predicted card probability values with cut off 0.5 and original card data
confusion1<-table(prob1>0.5,train_datal$card)
confusion1
Accuracy1<-sum(diag(confusion1)/sum(confusion1))
Accuracy1
#created variable (pred_values) with if predicted p value is greater than 0.5 put "yes" else "no"
pred_values1 <- ifelse(prob1>=0.5,"yes","no")
#Added column of (pred_values) to the original data set
train_datal[,"pred_values1"] <- pred_values1
View(train_datal[,c(1,7)]) #Just to compare actual data and predicted data
#Receiver Operating Characteristic (ROC) Curve Analysis
ROCR_Prediction1<-prediction(prob1,train_datal$card)
ROCR_Performance1<-performance(ROCR_Prediction1,'tpr','fpr')
plot(ROCR_Performance1,colorize=T,text.adj=c(-0.2,1.7),print.cutoffs.at=seq(0.1,by=0.1))
str(ROCR_Performance1)
ROCR_cutoff1 <- data.frame(cut_off = ROCR_Performance1@alpha.values[[1]],fpr=ROCR_Performance1@x.values,tpr=ROCR_Performance1@y.values)
colnames(ROCR_cutoff1) <- c("cut_off","FPR","TPR")
confusion2<-table(prob1>0.23,train_datal$card)
confusion2
Accuracy<-sum(diag(confusion2)/sum(confusion2))
Accuracy
